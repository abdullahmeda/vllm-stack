# Example values file for LoRA adapter deployment
# This file shows how to configure LoRA adapters in the production-stack Helm chart
servingEngineSpec:
  runtimeClassName: ""
  strategy:
    type: Recreate
  # vllmApiKey is the API key for the vLLM server. It is used to authenticate the vLLM server.
  # vllmApiKey: <your-vllm-api-key>
  modelSpec:
    - name: "llama3"
      repository: "vllm/vllm-openai"
      tag: "latest"
      modelURL: "meta-llama/Llama-3.1-8B-Instruct"
      enableLoRA: true

      # Option 1: Direct token
      # hf_token: <your-hf-token>

      # OR Option 2: Secret reference (RECOMMENDED)
      hf_token:
        secretName: "huggingface-credentials"
        secretKey: "HUGGING_FACE_HUB_TOKEN"

      # Other vLLM configs if needed
      vllmConfig:
        enablePrefixCaching: true
        maxModelLen: 4096
        dtype: "bfloat16"
        v1: 1
        extraArgs: ["--disable-log-requests", "--gpu-memory-utilization", "0.9"]

      # Mount Hugging Face credentials and configure LoRA settings
      env:
        - name: VLLM_ALLOW_RUNTIME_LORA_UPDATING
          value: "True"

      replicaCount: 1

      # Resource requirements for Llama-3.1-8b
      requestCPU: 8
      requestMemory: "32Gi"
      requestGPU: 1

      pvcStorage: "10Gi"
      pvcAccessMode:
        - ReadWriteOnce
  sidecar:
    image: "lmcache/lmstack-sidecar:latest"
    imagePullPolicy: "Always"

# Router configuration with resource limits to prevent OOM
routerSpec:
  resources:
    requests:
      cpu: 400m
      memory: 2Gi
    limits:
      memory: 2Gi

# Enable the lora controller (required for LoRA adapters)
loraController:
  enableLoraController: true
  image:
    repository: "lmcache/lmstack-lora-controller"
    tag: "latest"
    pullPolicy: "Always"

# Enable LoRA adapter functionality
loraAdapters:
    # Adapter 1: Topic Control
    - name: "llama3-nemoguard-topic-adapter"
      baseModel: "llama3"
      adapterSource:
        type: "huggingface"
        adapterName: "llama-3.1-nemoguard-8b-topic-control"
        repository: "nvidia/llama-3.1-nemoguard-8b-topic-control"
        credentials:
          secretName: "huggingface-credentials"
          secretKey: "HUGGING_FACE_HUB_TOKEN"
      loraAdapterDeploymentConfig:
        algorithm: "default"
        replicas: 1

    # Adapter 2: Roblox Guard
    - name: "llama3-robloxguard-adapter"
      baseModel: "llama3"
      adapterSource:
        type: "huggingface"
        adapterName: "Llama-3.1-8B-Instruct-RobloxGuard-1.0"
        repository: "Roblox/Llama-3.1-8B-Instruct-RobloxGuard-1.0"
        credentials:
          secretName: "huggingface-credentials"
          secretKey: "HUGGING_FACE_HUB_TOKEN"
      loraAdapterDeploymentConfig:
        algorithm: "default"
        replicas: 1
